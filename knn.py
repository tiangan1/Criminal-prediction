# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.


## KNN
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from warnings import filterwarnings
filterwarnings('ignore')

data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Criminal_Prediction/NYPD_Complaint_Sample_Data.csv')
data = data.drop(columns=['Unnamed: 0'])
print('We have total {} observations and {} variables.'.format(data.shape[0], data.shape[1]))

rename_dic = rename_dic = {'CMPLNT_NUM':'COMPLAINT_NUMBER','CMPLNT_FR_DT':'COMPLAINT_FROM_DATE','CMPLNT_FR_TM':'COMPLAINT_FROM_TIME',
              'CMPLNT_TO_DT':'COMPLAINT_TO_DATE','CMPLNT_TO_TM':'COMPLAINT_TO_TIME',
              'ADDR_PCT_CD' : 'ADDRESS_PRECINCT_CODE','RPT_DT' : 'REPORT_DT',
              'KY_CD' : 'OFFENSE_CLASS_CODE', 'OFNS_DESC':'OFFENSE_DESCRIBE',
              'PD_CD' : 'INTERNAL_CLASS_CODE','PD_DESC':'INTERNAL_DESCRIBE',
              'CRM_ATPT_CPTD_CD':'CRIME_ATTEMPT_COMPLETED_CODE','LAW_CAT_CD' : 'LEVEL_OF_OFFENSE',
              'BORO_NM':'BOROUGH_NAME','LOC_OF_OCCUR_DESC':'LOCATION_OF_OCCURRENCE_DESCRIBE',
              'PREM_TYP_DESC':'PREMIUM_TYPE_DESCRIBE','JURIS_DESC':'JURISDICTION_DESCRIBE',
              'PARKS_NM':'PARKS_NAME','HOUSING_PSA':'HOUSING_POLICE_SERVICE_AREA',
              'X_COORD_CD':'X_COORDINATE_CODE','Y_COORD_CD':'Y_COORDINATE_CODE',
              'SUSP_AGE_GROUP':'SUSPECT_AGE_GROUP','SUSP_RACE':'SUSPECT_RACE','SUSP_SEX':'SUSPECT_SEX',
              'PATROL_BORO':'PATROL_BOROUGH','VIC_AGE_GROUP':'VICTIM_AGE_GROUP','VIC_RACE':'VICTIM_RACE','VIC_SEX':'VICTIM_SEX',}

data= data.rename(columns = rename_dic).iloc[:,0:]
data.head()

data.COMPLAINT_FROM_TIME = pd.to_datetime(data.COMPLAINT_FROM_TIME, format='%H:%M:%S')
type(data.COMPLAINT_FROM_TIME[0])
data.COMPLAINT_FROM_DATE = pd.to_datetime(data.COMPLAINT_FROM_DATE, format='%Y-%m-%d')
type(data.COMPLAINT_FROM_DATE[0])

data = data.drop(columns=['PARKS_NAME', 'HADEVELOPT','X_COORDINATE_CODE','Y_COORDINATE_CODE', 
                          'HOUSING_POLICE_SERVICE_AREA', 'TRANSIT_DISTRICT','COMPLAINT_NUMBER',
                          'STATION_NAME', 'Lat_Lon', 'LOCATION_OF_OCCURRENCE_DESCRIBE',
                          'COMPLAINT_TO_DATE', 'COMPLAINT_TO_TIME','PATROL_BOROUGH',
                          'REPORT_DT','OFFENSE_DESCRIBE','INTERNAL_DESCRIBE','CRIME_ATTEMPT_COMPLETED_CODE',
                          'PREMIUM_TYPE_DESCRIBE','JURISDICTION_DESCRIBE','JURISDICTION_CODE']).dropna().reset_index(drop=True)
for i in data.columns:
    data = data[data[i] != 'UNKNOWN']
data = data.reset_index(drop=True)
data

crime = data[data.LEVEL_OF_OFFENSE!= 'VIOLATION'].reset_index(drop=True)
crime

susp_age = {'<18':0, '18-24':1, '25-44':2,
            '45-64':3, '65+':4}
crime.SUSPECT_AGE_GROUP = crime.SUSPECT_AGE_GROUP.replace(susp_age)

susp_race = {'AMERICAN INDIAN/ALASKAN NATIVE' : 0,
             'ASIAN / PACIFIC ISLANDER' : 1,
             'BLACK HISPANIC' : 2, 'WHITE' : 3,
             'WHITE HISPANIC' : 4, 'BLACK' : 5}

crime.SUSPECT_RACE = crime.SUSPECT_RACE.replace(susp_race)

susp_sex = {'M':1, 'F':0, 'U':2}
crime.SUSPECT_SEX = crime.SUSPECT_SEX.replace(susp_sex)

vic_age = {'<18':0, '18-24':1, '25-44':2,
            '45-64':3, '65+':4}
crime.VICTIM_AGE_GROUP = crime.VICTIM_AGE_GROUP.replace(vic_age)

vic_race = {'AMERICAN INDIAN/ALASKAN NATIVE' : 0,
             'ASIAN / PACIFIC ISLANDER' : 1,
             'BLACK HISPANIC' : 2, 'WHITE' : 3,
             'WHITE HISPANIC' : 4, 'BLACK' : 5}
crime.VICTIM_RACE = crime.VICTIM_RACE.replace(vic_race)

vic_sex = {'M':1, 'F':0,'D':2,'E':3}
crime.VICTIM_SEX = crime.VICTIM_SEX.replace(vic_sex)

BORO_NM = {'MANHATTAN' : 0,'BROOKLYN' : 1,'QUEENS' : 2,'BRONX' : 3, 'STATEN ISLAND' : 4 }
crime.BOROUGH_NAME = crime.BOROUGH_NAME.replace(BORO_NM)

lvl_offense = {'MISDEMEANOR':-1, 'FELONY':1}
crime.LEVEL_OF_OFFENSE = crime.LEVEL_OF_OFFENSE.replace(lvl_offense)

crime.COMPLAINT_FROM_TIME = crime.COMPLAINT_FROM_TIME.dt.hour
crime.COMPLAINT_FROM_DATE = crime.COMPLAINT_FROM_DATE.dt.year

crime

import plotly.express as px
fig = px.parallel_coordinates(crime, color='LEVEL_OF_OFFENSE',
                              dimensions=['OFFENSE_CLASS_CODE', 'BOROUGH_NAME', 'SUSPECT_AGE_GROUP', 'SUSPECT_RACE', 'SUSPECT_SEX', 
                                          'VICTIM_AGE_GROUP', 'VICTIM_RACE', 'VICTIM_SEX','Latitude','Longitude'],
                              color_continuous_scale=px.colors.diverging.Tealrose,
                              color_continuous_midpoint=2)
fig.show()

import matplotlib.pyplot as plt
import seaborn as sns
h = sns.pairplot(crime.loc[:, ['LEVEL_OF_OFFENSE','SUSPECT_AGE_GROUP', 'SUSPECT_RACE', 'SUSPECT_SEX','VICTIM_AGE_GROUP', 'VICTIM_RACE', 'VICTIM_SEX','ADDRESS_PRECINCT_CODE']],hue='LEVEL_OF_OFFENSE', markers=['o', 's'])
h.fig.set_size_inches(12,12)

data1 = crime
data1 = data1.drop(columns=['OFFENSE_CLASS_CODE', 'INTERNAL_CLASS_CODE'])

from sklearn.preprocessing import Normalizer, StandardScaler
Norm=Normalizer(data1)

data2 =data1.drop(columns=['LEVEL_OF_OFFENSE'])

scaler = StandardScaler()
norm_data = scaler.fit_transform(data2)

# compute_measure.py
import numpy as np
import pandas as pd
import math

def compute_measure(predicted_label, true_label):
    t_idx = (predicted_label == true_label)
    f_idx = np.logical_not(t_idx)

    p_idx = (true_label > 0)
    n_idx = np.logical_not(p_idx)

    tp = np.sum(np.logical_and(t_idx, p_idx)) #True Positive
    tn = np.sum(np.logical_and(t_idx, n_idx)) #True Negative

    #FP: original negative but classified as positive
    #FN: original positive but classified as negative

    fp = np.sum(n_idx) - tn
    fn = np.sum(p_idx) - tp

    tp_fp_tn_fn_list = []
    tp_fp_tn_fn_list.append(tp)
    tp_fp_tn_fn_list.append(fp)
    tp_fp_tn_fn_list.append(tn)
    tp_fp_tn_fn_list.append(fn)
    tp_fp_tn_fn_list = np.array(tp_fp_tn_fn_list)

    tp = tp_fp_tn_fn_list[0]
    fp = tp_fp_tn_fn_list[1]
    tn = tp_fp_tn_fn_list[2]
    fn = tp_fp_tn_fn_list[3]

    with np.errstate(divide='ignore'):
        sen = (1.0 * tp) / (tp + fn) * 100

    with np.errstate(divide='ignore'):
        spc = (1.0 * tn) / (tn + fp) * 100
  
    with np.errstate(divide='ignore'):
        ppr = (1.0 * tp) / (tp + fp) * 100

    with np.errstate(divide='ignore'):
        npr = (1.0 * tn) / (tn + fn) * 100

    acc = (tp + tn) * 1.0 / (tp + fp + tn + fn) * 100

    f1_score = 2 * tp / (2 * tp + fp + fn)

    d_idx = np.log2(1 + acc / 100) + np.log2(1 + (sen + spc) / 200)

    ans = []
    ans.append(acc)
    ans.append(sen)
    ans.append(spc)
    ans.append(ppr)
    ans.append(npr)
    ans.append(f1_score)
    ans.append(d_idx)

    ans_df = pd.DataFrame(ans, index=['accuracy', 'sensitivity', 
                                      'specificity', 'ppr', 'npr',
                                      'F-1 score', 'd-index'],
                          columns=['Score']).fillna(0).round(2)

    return ans_df

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from collections import Counter
from imblearn.over_sampling import SMOTE, RandomOverSampler

X_train,X_test,y_train,y_test=train_test_split(norm_data,data1['LEVEL_OF_OFFENSE'],
                                               test_size=0.3,random_state=42)

ros = RandomOverSampler(random_state=42)
X_train_res, y_train_res = ros.fit_sample(X_train, y_train)

print('Resampled training dataset shape {}'.format(Counter(y_train_res)))

k=[3,5,7,10]
for i in k:
  knn= KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train_res,y_train_res)
  y_pred= knn.predict(X_test)
  print('k= ' +str(i))
  print(compute_measure(y_pred,y_test))
  #print(classification_report(y_test,y_pred))
  #print('Accurancy:', metrics.accuracy_score(y_test,y_pred))

compute_measure(y_pred,y_test)

from sklearn import svm

## build a svm object sequence
models = (svm.SVC(kernel='linear'),
          svm.SVC(kernel='sigmoid'),
          svm.SVC(kernel='rbf', gamma='scale'),
          svm.SVC(kernel='poly', degree=3))

classification_measure_={}
for clf in models:
    clf.fit(X_train_res,y_train_res)
    predict_test_data_label = clf.predict(X_test)
    measure = compute_measure(predict_test_data_label,y_test)
    classification_measure_[clf.kernel]=measure
    print(clf.kernel)
    print(measure)
    print()

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split data
training_data, test_data, training_data_label, test_data_label \
= train_test_split(norm_data, data1['LEVEL_OF_OFFENSE'], 
                   test_size = 0.3, random_state=45)

# Set parameters
parameters = {'solver':('adam','lbfgs','sgd'),
              'activation':('relu','tanh','logistic'),
              'hidden_layer_sizes':[(20,),(100,2),(200,3),(300,)],
              'learning_rate':('constant','adaptive','invscaling')}
# Set MLP classifier
MLP = MLPClassifier()

# Set k_fold
k_fold_ = 5

# Build model to find best parameter
gs_clf = GridSearchCV(MLP, parameters, cv=k_fold_)
gs_clf.fit(training_data, training_data_label)
para_ = gs_clf.best_params_
print("\nParameters we use in MLP:\n")
print(para_)

# reset MLP model with best parameter
clf = MLPClassifier(activation=para_['activation'], 
                    hidden_layer_sizes=para_['hidden_layer_sizes'],
                    learning_rate=para_['learning_rate'],
                    solver=para_['solver'])

clf.fit(training_data, training_data_label)
predict_test_data_label = clf.predict(test_data)
print(classification_report(test_data_label, predict_test_data_label))

compute_measure(predict_test_data_label, test_data_label)